download wiktionary article dump from https://wikimedia.mirror.clarkson.edu/enwiktionary/20231020/
place in ./source-data-ignore
decompress with bzip2 -d
$ mkdir ./source-data-ignore/pages
$ cd  ./source-data-ignore/pages
$ csplit -k -n 10 ../enwiktionary-20231020-pages-articles.xml '/.*<page>.*/' '{8705923}' #Hi Zalgo!
$ for i in 1 2 3 4 5 6 7 8 9 0; do for j in 1 2 3 4 5 6 7 8 9 0; do for k in 1 2 3 4 5 6 7 8 9 0; do echo ${i}${j}${k}; mkdir ${i}${j}${k}; mv xx*${i}${j}${k} ./${i}${j}${k}/; done; done; done
$ mkdir ../latin-pages
$ for i in *; do mkdir ../latin-pages/$i; done
$ for i in *; do echo $i; for j in `grep -srl '==Latin==' $i`; do mv $j ../latin-pages/${j}.xml; done; done

8.2G	../enwiktionary-20231020-pages-articles.xml
4.7G	../latin-pages
 15M	../latin-pages-only.xml
 31G	../pages

$ cd ..
$ rm -rf pages
$ mkdir lemmata
$ cd lemmata
$ for i in ../latin-pages/*; do echo $i; for j in $i/*.xml; do ../../source/python/latin-from-pages.py $j; done; done #this takes a portion of a day
$ mkdir ../lemmata-prima
$ for i in `grep -srl 'la-[a-z]' .`; do mv $i ../lemmata-prima/$i; done
$ mkdir ../analysis
$ rm ../analysis/alltext.txt; for i in ../lemmata-prima/*.txt ../lemmata/*.txt; do echo $i; cat $i | grep . >> ../analysis/alltext.txt; done #this takes maybe 20 minutes or so
$ cd ../analysis
$ sed 's/{{/\n{{/g' alltext.txt > nltext.txt
$ ../../source/python/clean-alltext.py nltext.txt > cleaned.txt
$ ../../source/python/parse-alltext.py cleaned.txt  > parsed.csv
$ ../../source/python/versifier.py parsed.csv 10000 

